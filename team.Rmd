---
title: "lab5"
author: "Adam VanGorp and Mitchell Kazin"
date: "11/4/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Github Repository: https://github.com/Cygorp/ds202_lab5

## 1. 
``` {r}
diabetes<-read.table('diabetes.txt',header=TRUE)
head(diabetes)
```
## 2.
``` {r, results = 'hide'}
library(tidyverse)
diabetes[diabetes == ''] = NA
droplevels(diabetes)
```
```{r}
head(diabetes)
str(diabetes)
```
## 3.
``` {r}
diabetes_reduced <- select(diabetes, -c(id, bp.2s, bp.2d))
```

## 4.
```{r}
index.na1=which(apply(is.na(diabetes_reduced), 1, any) == TRUE)
diabetes_clean <- diabetes_reduced[-index.na1,]
nrow(diabetes_clean)
```

## 5.
``` {r}
index.na2=which(apply(is.na(diabetes_clean), 1, any) == TRUE)
index.na2
```
To see if all rows that have missing values were dropped we just have to run the given line of code again. If there are no results then there are no missing values. 

## 6.
Taking the log value of glyhb would make the data less right-skewed and more symmetric. A potential downside is information about the drastic differences between some peoples glyhb values.

## 7.
``` {r}
ggplot(diabetes_clean, aes(glyhb)) + geom_histogram() + ggtitle("Histogram of glyhb")
diabetes_clean$glyhb_star = log(diabetes_clean$glyhb)
ggplot(diabetes_clean, aes(glyhb_star)) + geom_histogram() + ggtitle("Histogram of Log(glyhb)")

```

## 8.
The larger the frame, the higher the glyhb_star mean. Buckingham has a slightly higher glyhb_star average than Louisa. Men are more likely to have a higher glyhb_star than women.
``` {r}
diabetes_clean %>% group_by(frame) %>% summarise(mean.glyhb = mean(glyhb_star))
diabetes_clean %>% group_by(location) %>% summarise(mean.glyhb = mean(glyhb_star))
diabetes_clean %>% group_by(gender) %>% summarise(mean.glyhb = mean(glyhb_star))
```

## 10.
``` {r}
 diabetes_clean %>% group_by(frame,location) %>% summarise (mean.glyhb_star= mean(glyhb_star)) %>% ggplot(aes(x = frame, y = mean.glyhb_star, color = location)) + geom_point() + ggtitle("Average glyhb_star by Frame")
```

## 11.
The only trend that is relatively clear from these graphs is that as ratio increases, glyhb_star will also increase.
``` {r}
ggplot(diabetes_clean, aes(ratio, glyhb_star, color = gender)) + geom_point() + ggtitle("Glyhb_star vs Ratio")
ggplot(diabetes_clean, aes(bp.1s, glyhb_star, color = gender)) + geom_point() + ggtitle("Glyhb_star vs Bp.1s")
ggplot(diabetes_clean, aes(age, glyhb_star, color = gender)) + geom_point() + ggtitle("Glyhb_star vs Age")
ggplot(diabetes_clean, aes(weight, glyhb_star, color = gender)) + geom_point() + ggtitle("Glyhb_star vs Weight")
```

## 12.
In order to see patterns in the data better, it may be helpful to facet by gender as well. Another way to visualize the data is with bar graphs.
``` {r}
ggplot(diabetes_clean,aes(y=hip,x=waist, alpha=.5)) + geom_point() + facet_wrap(gender~frame) + ggtitle("Hip versus Waist seperated by Gender and Frame")
ggplot(diabetes_clean,aes(y=hip,x=waist)) + geom_col() + facet_wrap(gender~frame) + ggtitle("Hip versus Waist seperated by Gender and Frame")
```

## 13.
The gather function takes data that is across multiple columns and condenses it into one column in the form of key value pairs. The spread function takes single column data and expands it over multiple columns based on the key value pairs of the first column.

## 14.
They are complementary function in the sense that you can gather the data and then spread it to get back the original data and vice versa.

## 15.
The high F-Statistic tells us that fitting a model is better than no model. The adjusted R-squared is somewhat low but it tells us that roughly half of the variance in glyhb_star can be explained by the chosen variables. Frame appears to be a poor predictor for glyhb_star due to the high p-values so they could be discarded in a future model. Ratio appears to have to the strongest correlation with glyhb_star due to its low p-value and a higher regression coefficient than other variables. Overall a linear model is a decent approach in this case, but more models would have to be explored to see if linear is the best choice.
``` {r}
fit = lm(glyhb_star ~stab.glu + age + waist + ratio+ factor(frame),data=diabetes_clean)
 summary(fit)
```
## 16.
The coefficients for stab.glu, age, and waist are all similar and relatively low values meaning as they increase, glyhb_star only slightly increases. Ratio has the largest coeffiecient changes in ratio lead to largest changes in glyhb_star. The different frames have large coefficient values but since they are factors it essentially means that the intercept changes for the given frame.

## 17.
The output below is the predicted value from our model.
```{r}
new_diabetes = data.frame(90, 35, 30, 5.1, 'small')
names(new_diabetes) = c("stab.glu", "age", "waist", "ratio", "frame")
predict(fit, new_diabetes)
```

## 18.
Inference is when you are looking to infer the effect of how an output is generated as a function of the data. Whereas prediction is when you are trying to choose a correct identifier from a set of outcomes. Inference is looking at the effect of x on y and prediction wants to see how often x is right.

## 19.
A linear regression model is better than a knn regression model when trying to fit and estimate a small number of coefficients.A disadvantage of a linear regression compared to knn is that linear regressions make strong assumptions about the form of f(x). This means if the relationship isn't linear the resulting model can be misleading/ wont fit well. A knn regression model doesn't assume an explicit form of f(x).

## 20.
I believe that I had a solid grasp of what data science is from taking 201 before this class so my views on data science haven't changed, but I do believe that my knowledge on and skills on the subject have grown. 

The most suprising part about DS to us was how many diferrent ways there are to view and manipulate data. There isn't always a "right" answer how to do things and methods vary from person to person.The most challenging part and the most enjoyable part of this course had to deal with collaborating on lab assingments and the group project. Working with others made the class more enjoyable but learning git and github to collaborate was challenging but rewarding. 




